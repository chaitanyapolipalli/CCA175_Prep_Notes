#1
/**************/
/**Problem 1**/
/**************/

Data is available in HDFS file system under /public/crime/csv
You can check properties of files using hadoop fs -ls -h /public/crime/csv
Structure of data (ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location)
File format - text file
Delimiter - “,”
Get monthly count of primary crime type, sorted by month in ascending and number of crimes per type in descending order
Store the result in HDFS path /user/<YOUR_USER_ID>/solutions/solution01/crimes_by_type_by_month
Output File Format: TEXT
Output Columns: Month in YYYYMM format, crime count, crime type
Output Delimiter: \t (tab delimited)
Output Compression: gzip


/*Using RDD''s*/
spark-shell --master yarn --conf spark.ui.port=14251 --num-executors=4 --executor-memory=2G --executor-cores 2

val crimeData = sc.textFile("/public/crime/csv")

val crimeHeader = crimeData.first

val crimeMap = crimeData.filter(c=>c != crimeHeader)

val crime = crimeMap.map(rec=> {
(rec.split(",")(2).split(" ")(0).replace("/","").substring(4,8).concat(rec.split(",")(2).split(" ")(0).replace("/","").substring(0,2)).toInt, rec.split(",")(5))
})

val crimeKeys = crime.map(rec => {(rec,1)}).reduceByKey((t, v) => t+v).map(c=> ((c._1._1, -c._2),c._1._1 + "\t" + c._2 + "\t" + c._1._2)).sortByKey().map(rec => rec._2)

crimeKeys.coalesce(1).saveAsTextFile("/user/chaitanyapolipalli/Dgadiraju_Problems/Problem_1_RDD",classOf[org.apache.hadoop.io.compress.GzipCodec])

/*Using Data Frames*/

spark-shell --master yarn --conf spark.ui.port=14521 --num-executors 4 --executor-memory 2G --executor-cores 2

val crimeData = sc.textFile("/public/crime/csv")
val crimeHeader = crimeData.first
val crimeNoHeader = crimeData.filter(c=>c != crimeHeader) 

val crime = crimeNoHeader.map(rec=> {
(rec.split(",")(2).split(" ")(0).replace("/","").substring(4,8).concat(rec.split(",")(2).split(" ")(0).replace("/","").substring(0,2)).toInt, rec.split(",")(5))
}).toDF("Date","Primary_Type")

crime.registerTempTable("crime")

val crim_count_per_monthDF = sqlContext.sql("select Date, count(1) crime_count, Primary_Type from crime group by Primary_Type, Date order by Date, crime_count desc")

val crimeRDD = crim_count_per_monthDF.rdd
val result = crimeRDD.map( c=> {c.mkString("\t")})

result.coalesce(1).saveAsTextFile("/user/chaitanyapolipalli/Dgadiraju_Problems/Problem_1_DF",classOf[org.apache.hadoop.io.compress.GzipCodec])

To validate any gzip compressed data, you cannot view it directly. Copy zipped file from hdfs to local using -get command then use “gunzip file_name” then data will be unzipped then you view data and validate

hadoop fs - get /user/chaitanyapolipalli/Dgadiraju_Problems/Problem_1_DF .

/*In local*/

cd to Problem_1_DF folder.
gunzip part-00000.gz

This will work only with Text file formats.

With other fileformats, save them to rdd and then load using sc variable and read data.
