# Instructions
# List the names of the Top 5 products by revenue ordered on '2013-07-26'. Revenue is considered only for COMPLETE and CLOSED orders.

# Data Description
# retail_db data is available in HDFS at /public/retail_db

# retail_db data information:

# Source directories: 
# /public/retail_db/orders 
# /public/retail_db/order_items 
# /public/retail_db/products
# Source delimiter: comma(",")
# Source Columns - orders - order_id, order_date, order_customer_id, order_status
# Source Columns - order_itemss - order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price
# Source Columns - products - product_id, product_category_id, product_name, product_description, product_price, product_image
# Output Requirements
# Target Columns: order_date, order_revenue, product_name, product_category_id
# Data has to be sorted in descending order by order_revenue
# File Format: text
# Delimiter: colon (:)
# Place the output file in the HDFS directory
# /user/`whoami`/problem7/solution/
# Replace `whoami` with your OS user name
# End of Problem


spark-shell --master yarn --conf spark.ui.port=15241 --num-executors 4 --executor-cores 2 --executor-memory 3G

val orders = sc.textFile("/public/retail_db/orders")
val order_items = sc.textFile("/public/retail_db/order_items")
val products = sc.textFile("/public/retail_db/products")

val ordersDF = orders.map(rec=> {
(rec.split(",")(0),rec.split(",")(1),rec.split(",")(2),rec.split(",")(3))
}).toDF("order_id","order_date", "order_customer_id", "order_status")
val order_itemsDF = order_items.map(rec=> {
(rec.split(",")(0),rec.split(",")(1),rec.split(",")(2),rec.split(",")(3),rec.split(",")(4),rec.split(",")(5))
}).toDF("order_item_id", "order_item_order_id", "order_item_product_id", "order_item_quantity", "order_item_subtotal", "order_item_product_price")
val productsDF = products.map(rec=> {
(rec.split(",")(0),rec.split(",")(1),rec.split(",")(2),rec.split(",")(3),rec.split(",")(4),rec.split(",")(5))
}).toDF("product_id", "product_category_id", "product_name", "product_description", "product_price", "product_image")

ordersDF.registerTempTable("orders")
order_itemsDF.registerTempTable("order_items")
productsDF.registerTempTable("products")

sqlContext.setConf("spark.sql.shuffle.partitions","2")

val query = sqlContext.sql("select o.order_date, sum(order_item_subtotal) order_revenue ,p.product_name,p.product_category_id from orders o join order_items oi on o.order_id = oi.order_item_order_id join products p on p.product_id = oi.order_item_product_id where order_date like '%2013-07-26%' and order_status IN ('COMPLETE','CLOSED') group by product_category_id, order_date, product_name order by order_revenue desc  limit 5")

val result_rdd = query.rdd

val resultMap = result_rdd.map(rec=> rec.mkString(":"))

resultMap.coalesce(1).saveAsTextFile("/user/chaitanyapolipalli/Practice_exam/problem7/solution/")
