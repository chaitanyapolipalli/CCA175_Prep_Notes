# Import orders table from mysql as text file to the destination /user/cloudera/problem4/text. Fields should be terminated by a tab character ("\t") character and lines should be terminated by new line character ("\n"). 

sqoop import \
--connect jdbc:mysql://ms.itversity.com/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-textfile \
--target-dir /user/chaitanyapolipalli/ArunBlogPractice/problem4/text \
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
-m 1

Import orders table from mysql  into hdfs to the destination /user/cloudera/problem4/avro. File should be stored as avro file.

sqoop import \
--connect jdbc:mysql://ms.itversity.com/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-avrodatafile \
--target-dir /user/chaitanyapolipalli/ArunBlogPractice/problem4/avro \
-m 1

Import orders table from mysql  into hdfs  to folders /user/cloudera/problem4/parquet. File should be stored as parquet file.

sqoop import \
--connect jdbc:mysql://ms.itversity.com/retail_db \
--username retail_user \
--password itversity \
--table orders \
--as-parquetfile \
--target-dir /user/chaitanyapolipalli/ArunBlogPractice/problem4/parquet \
-m 1


# Transform/Convert data-files at /user/cloudera/problem4/avro and store the converted file at the following locations and file formats
# save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem4/parquet-snappy-compress
# save the data to hdfs using gzip compression as text file at /user/cloudera/problem4/text-gzip-compress
# save the data to hdfs using no compression as sequence file at /user/cloudera/problem4/sequence
# save the data to hdfs using snappy compression as text file at /user/cloudera/problem4/text-snappy-compress

spark-shell --master yarn --conf spark.ui.port=11241 --packages com.databricks:spark-avro_2.10:2.0.1
import com.databricks.spark.avro._

val ordersDF = sqlContext.read.avro("/user/chaitanyapolipalli/ArunBlogPractice/problem4/avro")

sqlContext.setConf("spark.sql.parquet.compression.codec","snappy")

ordersDF.write.parquet("/user/chaitanyapolipalli/ArunBlogPractice/problem4/parquet-snappy-compress")


ordersDF.map(rec=> rec(0)+"\t"+rec(1)+"\t"+ rec(2)+"\t"+rec(3)).saveAsTextFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/text-gzip-compress", classOf[org.apache.hadoop.io.compress.GzipCodec])


ordersDF.map(rec=> (rec(0)+"\t"+rec(1)+"\t"+ rec(2)+"\t"+rec(3))).saveAsSequenceFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/sequence")

ordersDF.map(rec=> rec(0)+"\t"+rec(1)+"\t"+ rec(2)+"\t"+rec(3)).saveAsTextFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/text-snappy-compress",classOf[org.apache.hadoop.io.compress.SnappyCodec])

sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed")

# Transform/Convert data-files at /user/cloudera/problem4/parquet-snappy-compress and store the converted file at the following locations and file formats
# save the data to hdfs using no compression as parquet file at /user/cloudera/problem4/parquet-no-compress
# save the data to hdfs using snappy compression as avro file at /user/cloudera/problem4/avro-snappy

val snappyCompressedDF  = sqlContext.read.parquet("/user/chaitanyapolipalli/ArunBlogPractice/problem4/parquet-snappy-compress")

sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed")

snappyCompressedDF.write.parquet("/user/chaitanyapolipalli/ArunBlogPractice/problem4/parquet-no-compress")

sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

snappyCompressedDF.write.avro("/user/chaitanyapolipalli/ArunBlogPractice/problem4/avro-snappy")

# Transform/Convert data-files at /user/cloudera/problem4/avro-snappy and store the converted file at the following locations and file formats
# save the data to hdfs using no compression as json file at /user/cloudera/problem4/json-no-compress
# save the data to hdfs using gzip compression as json file at /user/cloudera/problem4/json-gzip

val avroSnappyDF = sqlContext.read.avro("/user/chaitanyapolipalli/ArunBlogPractice/problem4/avro-snappy")

avroSnappyDF.toJSON.saveAsTextFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/json-no-compress")

avroSnappyDF.toJSON.saveAsTextFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])


# Transform/Convert data-files at  /user/cloudera/problem4/json-gzip and store the converted file at the following locations and file formats
# save the data to as comma separated text using gzip compression at   /user/cloudera/problem4/csv-gzip

val jsonGzipDF = sqlContext.read.json("/user/chaitanyapolipalli/ArunBlogPractice/problem4/json-gzip")

jsonGzipDF.map(rec=> rec(0) + ","+ rec(1) + ","+rec(2) + ","+rec(3)).saveAsTextFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])


# Using spark access data at /user/cloudera/problem4/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem4/orc 

val sequenceRDD = sc.sequenceFile("/user/chaitanyapolipalli/ArunBlogPractice/problem4/sequence")

val sequenceDF = sequenceRDD.toDF()

sequenceDF.write.orc("/user/chaitanyapolipalli/ArunBlogPractice/problem4/orc")
